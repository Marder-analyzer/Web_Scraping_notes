{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432f0019",
   "metadata": {},
   "source": [
    "# Web Scraping Kapsamlı Rehberi - Bölüm 1-5\n",
    "\n",
    "## İçindekiler\n",
    "1. [Web Scraping Nedir?](#1-web-scraping-nedir)\n",
    "2. [Web Scraping için Kullanılan Model Türleri ve Özellikleri](#2-web-scraping-için-kullanılan-model-türleri-ve-özellikleri)\n",
    "3. [Site Analizi ve Veri Miktarı Belirleme](#3-site-analizi-ve-veri-miktarı-belirleme)\n",
    "4. [Hangi Sitede Hangi Model Kullanılmalı?](#4-hangi-sitede-hangi-model-kullanılmalı)\n",
    "5. [Web Scraping Engellemeleri ve Sorunlar](#5-web-scraping-engellemeleri-ve-sorunlar)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Web Scraping Nedir?\n",
    "\n",
    "### Tanım\n",
    "Web scraping (web kazıma), web sitelerinden otomatik olarak veri toplama işlemidir. İnsan gibi manuel olarak bilgi kopyalamak yerine, yazılımlar kullanarak sistematik ve hızlı veri çekme sürecidir.\n",
    "\n",
    "### Kullanım Alanları\n",
    "- **E-ticaret:** Rakip fiyat takibi, ürün karşılaştırma\n",
    "- **Veri Analizi:** Araştırma için veri toplama\n",
    "- **SEO:** Arama motoru sonuçlarını izleme\n",
    "- **Emlak:** Konut fiyatları ve trend analizi\n",
    "- **Sosyal Medya:** İçerik ve trend analizi\n",
    "- **Haber Toplama:** Otomatik haber agregasyonu\n",
    "\n",
    "### Yasal ve Etik Boyutlar\n",
    "\n",
    "**✅ Yasal Kullanım:**\n",
    "- Halka açık veriler\n",
    "- robots.txt'e uygun scraping\n",
    "- Sitede scraping ile ilgili açık izin\n",
    "- Kişisel kullanım için makul veri çekme\n",
    "\n",
    "**❌ Dikkat Edilmesi Gerekenler:**\n",
    "- Telif hakkı korumalı içerikler\n",
    "- Kişisel veriler (KVKK/GDPR)\n",
    "- Site kullanım şartlarına aykırılık\n",
    "- Sitenin performansını olumsuz etkileme\n",
    "- DDoS benzeri yoğun trafik oluşturma\n",
    "\n",
    "**Önemli:** Web scraping yapmadan önce:\n",
    "1. `robots.txt` dosyasını kontrol edin (örn: `site.com/robots.txt`)\n",
    "2. Kullanım şartlarını okuyun\n",
    "3. Makul bekleme süreleri (rate limiting) ekleyin\n",
    "4. API var mı kontrol edin (tercih edilir)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Web Scraping için Kullanılan Model Türleri ve Özellikleri\n",
    "\n",
    "### 2.1. HTTP İstemci Tabanlı (Request-Based)\n",
    "**Özellikler:**\n",
    "- Sadece HTTP istekleri gönderir\n",
    "- JavaScript çalıştırmaz\n",
    "- Hızlı ve hafif\n",
    "\n",
    "**Araçlar:** requests, httpx, urllib, aiohttp\n",
    "\n",
    "### 2.2. HTML Parser'lar\n",
    "**Özellikler:**\n",
    "- HTML'i parse eder ve yapılandırır\n",
    "- CSS seçicileri veya XPath kullanır\n",
    "\n",
    "**Araçlar:** BeautifulSoup, lxml, html5lib, parsel\n",
    "\n",
    "### 2.3. Headless Browser Tabanlı\n",
    "**Özellikler:**\n",
    "- Gerçek browser simülasyonu\n",
    "- JavaScript çalıştırır\n",
    "- Kullanıcı etkileşimi simüle eder\n",
    "\n",
    "**Araçlar:** Selenium, Playwright, Puppeteer\n",
    "\n",
    "### 2.4. Framework'ler\n",
    "**Özellikler:**\n",
    "- Komple scraping çözümü\n",
    "- Veri işleme pipeline'ı\n",
    "\n",
    "**Araçlar:** Scrapy, PySpider\n",
    "\n",
    "### 2.5. API Wrapper'ları\n",
    "**Özellikler:**\n",
    "- Belirli siteler için hazır çözümler\n",
    "\n",
    "**Araçlar:** ScraperAPI, Apify, Bright Data\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Site Analizi ve Veri Miktarı Belirleme\n",
    "\n",
    "### 3.1. Site Analizi Adımları\n",
    "\n",
    "#### Adım 1: Manuel İnceleme\n",
    "1. Tarayıcıda siteyi ziyaret edin\n",
    "2. Veri yapısını inceleyin\n",
    "3. Sayfa geçişlerini gözlemleyin\n",
    "\n",
    "#### Adım 2: robots.txt Kontrolü\n",
    "\\`\\`\\`\n",
    "https://example.com/robots.txt\n",
    "\\`\\`\\`\n",
    "\n",
    "#### Adım 3: Developer Tools İncelemesi\n",
    "1. F12 tuşuna basın\n",
    "2. Network sekmesine gidin\n",
    "3. İstekleri inceleyin\n",
    "\n",
    "#### Adım 4: HTML Yapısını Analiz\n",
    "CSS seçicileri ve ID'leri tespit edin\n",
    "\n",
    "### 3.2. Veri Miktarı Belirleme\n",
    "\n",
    "**Hesaplama örneği:**\n",
    "\\`\\`\\`python\n",
    "total_pages = 150\n",
    "items_per_page = 20\n",
    "total_items = total_pages * items_per_page  # 3000 ürün\n",
    "\\`\\`\\`\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hangi Sitede Hangi Model Kullanılmalı?\n",
    "\n",
    "### 4.1. Karar Ağacı\n",
    "\n",
    "\\`\\`\\`\n",
    "JavaScript gerekli mi?\n",
    "  ├─ EVET → Headless Browser (Selenium/Playwright)\n",
    "  └─ HAYIR → HTTP Client (requests) + Parser\n",
    "\n",
    "API endpoint var mı?\n",
    "  ├─ EVET → Doğrudan API kullan\n",
    "  └─ HAYIR → HTML parsing\n",
    "\n",
    "Proje büyüklüğü?\n",
    "  ├─ Küçük (<1000) → Basit script\n",
    "  ├─ Orta (1K-100K) → Scrapy\n",
    "  └─ Büyük (>100K) → Scrapy + Distributed\n",
    "\\`\\`\\`\n",
    "\n",
    "### 4.2. Site Tiplerine Göre Araç Seçimi\n",
    "\n",
    "#### Statik HTML Siteleri\n",
    "**Önerilen:** requests + BeautifulSoup\n",
    "\n",
    "\\`\\`\\`python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://example.com')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\\`\\`\\`\n",
    "\n",
    "#### JavaScript ile Yüklenen Siteler\n",
    "**Seçenek A:** API Endpoint kullan\n",
    "**Seçenek B:** Headless Browser (Playwright/Selenium)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Web Scraping Engellemeleri ve Sorunlar\n",
    "\n",
    "### 5.1. Temel Engelleme Mekanizmaları\n",
    "\n",
    "#### 1. robots.txt Kontrolü\n",
    "Sitenin hangi bölümlerinin taranabileceğini belirtir\n",
    "\n",
    "#### 2. Rate Limiting\n",
    "Belirli sürede yapılabilecek istek sayısı sınırı\n",
    "\n",
    "#### 3. User-Agent Kontrolü\n",
    "İsteğin hangi tarayıcıdan geldiğini kontrol eder\n",
    "\n",
    "#### 4. IP Bazlı Engelleme\n",
    "Aynı IP'den çok fazla istek gelirse engeller\n",
    "\n",
    "#### 5. Cookie ve Session Kontrolü\n",
    "Botlar genelde cookie kullanmaz\n",
    "\n",
    "#### 6. JavaScript Challenge\n",
    "JavaScript çalıştırarak bot tespiti yapar\n",
    "\n",
    "#### 7. CAPTCHA\n",
    "İnsan doğrulaması gerektirir\n",
    "\n",
    "### 5.2. Yaygın HTTP Hata Kodları\n",
    "\n",
    "| Kod | Anlamı | Çözüm |\n",
    "|-----|--------|-------|\n",
    "| 403 | Forbidden | Proxy, User-Agent değiştir |\n",
    "| 429 | Too Many Requests | Bekleme süresi ekle |\n",
    "| 503 | Service Unavailable | Daha yavaş scrape |\n",
    "| 401 | Unauthorized | Login, API key |\n",
    "\n",
    "---\n",
    "\n",
    "**Not:** Bu rehber Bölüm 1-5'i kapsamaktadır. Bölüm 6-10 için \"web_scraping_rehberi_bolum6-10.md\" dosyasına bakınız."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
