{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432f0019",
   "metadata": {},
   "source": [
    "# Web Scraping Kapsamlı Rehberi\n",
    "\n",
    "## İçindekiler\n",
    "1. [Web Scraping Nedir?](#1-web-scraping-nedir)\n",
    "2. [Web Scraping için Kullanılan Model Türleri ve Özellikleri](#2-web-scraping-için-kullanılan-model-türleri-ve-özellikleri)\n",
    "3. [Site Analizi ve Veri Miktarı Belirleme](#3-site-analizi-ve-veri-miktarı-belirleme)\n",
    "4. [Hangi Sitede Hangi Model Kullanılmalı?](#4-hangi-sitede-hangi-model-kullanılmalı)\n",
    "5. [Web Scraping Engellemeleri ve Sorunlar](#5-web-scraping-engellemeleri-ve-sorunlar)\n",
    "6. [Engelleme Çözme Yöntemleri](#6-engelleme-çözme-yöntemleri)\n",
    "7. [Model Karşılaştırması ve Proje Bazlı Seçim](#7-model-karşılaştırması)\n",
    "8. [Modellerin Detaylı Anlatımı](#8-modellerin-detaylı-anlatımı)\n",
    "9. [Her Model için Adım Adım Uygulama](#9-adım-adım-uygulamalar)\n",
    "10. [Ekler ve Kaynaklar](#10-ekler-ve-kaynaklar)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Web Scraping Nedir?\n",
    "\n",
    "### Tanım\n",
    "Web scraping (web kazıma), web sitelerinden otomatik olarak veri toplama işlemidir. İnsan gibi manuel olarak bilgi kopyalamak yerine, yazılımlar kullanarak sistematik ve hızlı veri çekme sürecidir.\n",
    "\n",
    "### Kullanım Alanları\n",
    "- **E-ticaret:** Rakip fiyat takibi, ürün karşılaştırma\n",
    "- **Veri Analizi:** Araştırma için veri toplama\n",
    "- **SEO:** Arama motoru sonuçlarını izleme\n",
    "- **Emlak:** Konut fiyatları ve trend analizi\n",
    "- **Sosyal Medya:** İçerik ve trend analizi\n",
    "- **Haber Toplama:** Otomatik haber agregasyonu\n",
    "\n",
    "\n",
    "**Önemli:** Web scraping yapmadan önce:\n",
    "1. `robots.txt` dosyasını kontrol edin (örn: `site.com/robots.txt`)\n",
    "2. Kullanım şartlarını okuyun\n",
    "3. Makul bekleme süreleri (rate limiting) ekleyin\n",
    "4. API var mı kontrol edin (tercih edilir çünkü her zaman daha sürdülebilir ve daha hızlıdır.)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Web Scraping için Kullanılan Model Türleri ve Özellikleri\n",
    "\n",
    "### 2.1. HTTP İstemci Tabanlı (Request-Based)\n",
    "**Özellikler:**\n",
    "- Sadece HTTP istekleri gönderir\n",
    "- JavaScript çalıştırmaz\n",
    "- Hızlı ve hafif\n",
    "\n",
    "**Araçlar:** requests, httpx, urllib, aiohttp\n",
    "\n",
    "### 2.2. HTML Parser'lar\n",
    "**Özellikler:**\n",
    "- HTML'i parse eder ve yapılandırır\n",
    "- CSS seçicileri veya XPath kullanır\n",
    "\n",
    "**Araçlar:** BeautifulSoup, lxml, html5lib, parsel\n",
    "\n",
    "### 2.3. Headless Browser Tabanlı\n",
    "**Özellikler:**\n",
    "- Gerçek browser simülasyonu\n",
    "- JavaScript çalıştırır\n",
    "- Kullanıcı etkileşimi simüle eder\n",
    "\n",
    "**Araçlar:** Selenium, Playwright, Puppeteer\n",
    "\n",
    "### 2.4. Framework'ler\n",
    "**Özellikler:**\n",
    "- Komple scraping çözümü\n",
    "- Veri işleme pipeline'ı\n",
    "\n",
    "**Araçlar:** Scrapy, PySpider\n",
    "\n",
    "### 2.5. API Wrapper'ları\n",
    "**Özellikler:**\n",
    "- Belirli siteler için hazır çözümler\n",
    "\n",
    "**Araçlar:** ScraperAPI, Apify, Bright Data\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Site Analizi ve Veri Miktarı Belirleme\n",
    "\n",
    "### 3.1. Site Analizi Adımları\n",
    "\n",
    "#### Adım 1: Manuel İnceleme\n",
    "1. Tarayıcıda siteyi ziyaret edin\n",
    "2. Veri yapısını inceleyin\n",
    "3. Sayfa geçişlerini gözlemleyin\n",
    "\n",
    "#### Adım 2: robots.txt Kontrolü\n",
    "\\`\\`\\`\n",
    "https://example.com/robots.txt\n",
    "\\`\\`\\`\n",
    "\n",
    "#### Adım 3: Developer Tools İncelemesi\n",
    "1. F12 tuşuna basın\n",
    "2. Network sekmesine gidin\n",
    "3. İstekleri inceleyin\n",
    "\n",
    "#### Adım 4: HTML Yapısını Analiz\n",
    "CSS seçicileri ve ID'leri tespit edin\n",
    "\n",
    "### 3.2. Veri Miktarı Belirleme\n",
    "\n",
    "**Hesaplama örneği:**\n",
    "\\`\\`\\`python\n",
    "total_pages = 150\n",
    "items_per_page = 20\n",
    "total_items = total_pages * items_per_page  # 3000 ürün\n",
    "\\`\\`\\`\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hangi Sitede Hangi Model Kullanılmalı?\n",
    "\n",
    "### 4.1. Karar Ağacı\n",
    "\n",
    "\\`\\`\\`\n",
    "JavaScript gerekli mi?\n",
    "  ├─ EVET → Headless Browser (Selenium/Playwright)\n",
    "  └─ HAYIR → HTTP Client (requests) + Parser\n",
    "\n",
    "API endpoint var mı?\n",
    "  ├─ EVET → Doğrudan API kullan\n",
    "  └─ HAYIR → HTML parsing\n",
    "\n",
    "Proje büyüklüğü?\n",
    "  ├─ Küçük (<1000) → Basit script\n",
    "  ├─ Orta (1K-100K) → Scrapy\n",
    "  └─ Büyük (>100K) → Scrapy + Distributed\n",
    "\\`\\`\\`\n",
    "\n",
    "### 4.2. Site Tiplerine Göre Araç Seçimi\n",
    "\n",
    "#### Statik HTML Siteleri\n",
    "**Önerilen:** requests + BeautifulSoup\n",
    "\n",
    "\\`\\`\\`python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://example.com')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\\`\\`\\`\n",
    "\n",
    "#### JavaScript ile Yüklenen Siteler\n",
    "**Seçenek A:** API Endpoint kullan\n",
    "**Seçenek B:** Headless Browser (Playwright/Selenium)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Web Scraping Engellemeleri ve Sorunlar\n",
    "\n",
    "### 5.1. Temel Engelleme Mekanizmaları\n",
    "\n",
    "#### 1. robots.txt Kontrolü\n",
    "Sitenin hangi bölümlerinin taranabileceğini belirtir\n",
    "\n",
    "#### 2. Rate Limiting\n",
    "Belirli sürede yapılabilecek istek sayısı sınırı\n",
    "\n",
    "#### 3. User-Agent Kontrolü\n",
    "İsteğin hangi tarayıcıdan geldiğini kontrol eder\n",
    "\n",
    "#### 4. IP Bazlı Engelleme\n",
    "Aynı IP'den çok fazla istek gelirse engeller\n",
    "\n",
    "#### 5. Cookie ve Session Kontrolü\n",
    "Botlar genelde cookie kullanmaz\n",
    "\n",
    "#### 6. JavaScript Challenge\n",
    "JavaScript çalıştırarak bot tespiti yapar\n",
    "\n",
    "#### 7. CAPTCHA\n",
    "İnsan doğrulaması gerektirir\n",
    "\n",
    "### 5.2. Yaygın HTTP Hata Kodları\n",
    "\n",
    "| Kod | Anlamı | Çözüm |\n",
    "|-----|--------|-------|\n",
    "| 403 | Forbidden | Proxy, User-Agent değiştir |\n",
    "| 429 | Too Many Requests | Bekleme süresi ekle |\n",
    "| 503 | Service Unavailable | Daha yavaş scrape |\n",
    "| 401 | Unauthorized | Login, API key |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Engelleme Çözme Yöntemleri\n",
    "\n",
    "### 6.1. Temel Yöntemler\n",
    "\n",
    "#### 1. User-Agent Rotation\n",
    "\\`\\`\\`python\n",
    "import requests\n",
    "import random\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "]\n",
    "\n",
    "headers = {'User-Agent': random.choice(user_agents)}\n",
    "response = requests.get(url, headers=headers)\n",
    "\\`\\`\\`\n",
    "\n",
    "#### 2. Rate Limiting\n",
    "\\`\\`\\`python\n",
    "import time\n",
    "import random\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    delay = random.uniform(1, 3)\n",
    "    time.sleep(delay)\n",
    "\\`\\`\\`\n",
    "\n",
    "#### 3. Proxy Kullanımı\n",
    "\\`\\`\\`python\n",
    "proxies = {\n",
    "    'http': 'http://proxy_ip:port',\n",
    "    'https': 'http://proxy_ip:port'\n",
    "}\n",
    "response = requests.get(url, proxies=proxies)\n",
    "\\`\\`\\`\n",
    "\n",
    "### 6.2. Headless Browser Teknikleri\n",
    "\n",
    "\\`\\`\\`python\n",
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\\`\\`\\`\n",
    "\n",
    "### 6.3. CAPTCHA Çözme\n",
    "- **2Captcha** servisi kullanımı\n",
    "- Manuel çözüm\n",
    "- CAPTCHA'dan kaçınma (daha iyi yöntem)\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Model Karşılaştırması\n",
    "\n",
    "### 7.1. Karşılaştırma Tablosu\n",
    "\n",
    "| Özellik | requests+BS4 | Scrapy | Selenium | Playwright |\n",
    "|---------|--------------|--------|----------|------------|\n",
    "| Hız | ⚡⚡⚡ | ⚡⚡⚡ | ⚡ | ⚡⚡ |\n",
    "| Kaynak | Minimal | Orta | Yüksek | Yüksek |\n",
    "| JavaScript | ❌ | ❌ | ✅ | ✅ |\n",
    "| Öğrenme | Kolay | Orta | Orta | Orta-Zor |\n",
    "\n",
    "### 7.2. Proje Bazlı Seçim\n",
    "\n",
    "**Küçük Proje (<1000 kayıt):** requests + BeautifulSoup\n",
    "**Orta Proje (1K-100K):** Scrapy\n",
    "**JavaScript Sitesi:** Playwright/Selenium\n",
    "**Cloudflare Korumalı:** Undetected ChromeDriver\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Modellerin Detaylı Anlatımı\n",
    "\n",
    "### 8.1. requests + BeautifulSoup\n",
    "\n",
    "**Kurulum:**\n",
    "\\`\\`\\`bash\n",
    "pip install requests beautifulsoup4 lxml\n",
    "\\`\\`\\`\n",
    "\n",
    "**Temel Kullanım:**\n",
    "\\`\\`\\`python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = requests.get('https://example.com')\n",
    "soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "title = soup.find('h1').text\n",
    "links = soup.find_all('a')\n",
    "\\`\\`\\`\n",
    "\n",
    "**Avantajları:**\n",
    "- ✅ Çok hızlı\n",
    "- ✅ Kolay öğrenme\n",
    "- ✅ Minimal kaynak\n",
    "\n",
    "**Dezavantajları:**\n",
    "- ❌ JavaScript desteği yok\n",
    "- ❌ Dinamik içerik yok\n",
    "\n",
    "### 8.2. Scrapy\n",
    "\n",
    "**Kurulum:**\n",
    "\\`\\`\\`bash\n",
    "pip install scrapy\n",
    "scrapy startproject myproject\n",
    "\\`\\`\\`\n",
    "\n",
    "**Temel Spider:**\n",
    "\\`\\`\\`python\n",
    "import scrapy\n",
    "\n",
    "class ProductSpider(scrapy.Spider):\n",
    "    name = 'products'\n",
    "    start_urls = ['https://example.com/products']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for product in response.css('div.product'):\n",
    "            yield {\n",
    "                'name': product.css('h3::text').get(),\n",
    "                'price': product.css('span.price::text').get()\n",
    "            }\n",
    "\\`\\`\\`\n",
    "\n",
    "**Avantajları:**\n",
    "- ✅ Asenkron, çok hızlı\n",
    "- ✅ Built-in middleware\n",
    "- ✅ Ölçeklenebilir\n",
    "\n",
    "### 8.3. Selenium\n",
    "\n",
    "**Kurulum:**\n",
    "\\`\\`\\`bash\n",
    "pip install selenium webdriver-manager\n",
    "\\`\\`\\`\n",
    "\n",
    "**Kullanım:**\n",
    "\\`\\`\\`python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://example.com')\n",
    "\n",
    "element = driver.find_element(By.CLASS_NAME, 'product')\n",
    "text = element.text\n",
    "\n",
    "driver.quit()\n",
    "\\`\\`\\`\n",
    "\n",
    "**Avantajları:**\n",
    "- ✅ JavaScript desteği\n",
    "- ✅ User interaction\n",
    "\n",
    "**Dezavantajları:**\n",
    "- ❌ Yavaş\n",
    "- ❌ Yüksek kaynak\n",
    "\n",
    "### 8.4. Playwright\n",
    "\n",
    "**Kurulum:**\n",
    "\\`\\`\\`bash\n",
    "pip install playwright\n",
    "playwright install\n",
    "\\`\\`\\`\n",
    "\n",
    "**Kullanım:**\n",
    "\\`\\`\\`python\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "with sync_playwright() as p:\n",
    "    browser = p.chromium.launch()\n",
    "    page = browser.new_page()\n",
    "    page.goto('https://example.com')\n",
    "    \n",
    "    title = page.inner_text('h1')\n",
    "    browser.close()\n",
    "\\`\\`\\`\n",
    "\n",
    "**Avantajları:**\n",
    "- ✅ Selenium'dan hızlı\n",
    "- ✅ Modern API\n",
    "- ✅ Network interception\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Adım Adım Uygulamalar\n",
    "\n",
    "### 9.1. requests + BeautifulSoup Örneği\n",
    "\n",
    "**Proje:** BBC News Scraper\n",
    "\n",
    "\\`\\`\\`python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_bbc_news():\n",
    "    url = \"https://www.bbc.com/news\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "    \n",
    "    articles = []\n",
    "    for item in soup.find_all('div', class_='gs-c-promo'):\n",
    "        title = item.find('h3')\n",
    "        if title:\n",
    "            articles.append({\n",
    "                'title': title.text.strip(),\n",
    "                'link': item.find('a')['href']\n",
    "            })\n",
    "    \n",
    "    return articles\n",
    "\n",
    "news = scrape_bbc_news()\n",
    "print(json.dumps(news, indent=2))\n",
    "\\`\\`\\`\n",
    "\n",
    "### 9.2. Scrapy Tam Proje\n",
    "\n",
    "\\`\\`\\`bash\n",
    "# Proje oluştur\n",
    "scrapy startproject ecommerce_scraper\n",
    "cd ecommerce_scraper\n",
    "scrapy genspider products example.com\n",
    "\n",
    "# Spider yazma (spiders/products.py)\n",
    "import scrapy\n",
    "\n",
    "class ProductsSpider(scrapy.Spider):\n",
    "    name = 'products'\n",
    "    start_urls = ['https://example.com/products']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for product in response.css('div.product'):\n",
    "            yield {\n",
    "                'name': product.css('h3::text').get(),\n",
    "                'price': product.css('span::text').get()\n",
    "            }\n",
    "        \n",
    "        next_page = response.css('a.next::attr(href)').get()\n",
    "        if next_page:\n",
    "            yield response.follow(next_page, self.parse)\n",
    "\n",
    "# Çalıştırma\n",
    "scrapy crawl products -o products.json\n",
    "\\`\\`\\`\n",
    "\n",
    "### 9.3. Selenium Tam Örnek\n",
    "\n",
    "\\`\\`\\`python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "\n",
    "def scrape_dynamic_site():\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get('https://example-shop.com/products')\n",
    "    \n",
    "    # JavaScript yüklensin\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'product')))\n",
    "    \n",
    "    # Scroll\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Veri topla\n",
    "    products = []\n",
    "    elements = driver.find_elements(By.CLASS_NAME, 'product')\n",
    "    \n",
    "    for element in elements:\n",
    "        name = element.find_element(By.TAG_NAME, 'h3').text\n",
    "        price = element.find_element(By.CLASS_NAME, 'price').text\n",
    "        products.append({'name': name, 'price': price})\n",
    "    \n",
    "    driver.quit()\n",
    "    return products\n",
    "\n",
    "products = scrape_dynamic_site()\n",
    "print(json.dumps(products, indent=2))\n",
    "\\`\\`\\`\n",
    "\n",
    "### 9.4. Playwright Async Örnek\n",
    "\n",
    "\\`\\`\\`python\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "\n",
    "async def scrape_products():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto('https://example.com/products')\n",
    "        \n",
    "        await page.wait_for_selector('.product')\n",
    "        \n",
    "        products = await page.locator('.product').all()\n",
    "        \n",
    "        data = []\n",
    "        for product in products:\n",
    "            name = await product.locator('h3').inner_text()\n",
    "            price = await product.locator('.price').inner_text()\n",
    "            data.append({'name': name, 'price': price})\n",
    "        \n",
    "        await browser.close()\n",
    "        return data\n",
    "\n",
    "products = asyncio.run(scrape_products())\n",
    "\\`\\`\\`\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Ekler ve Kaynaklar\n",
    "\n",
    "### 10.1. Faydalı Kütüphaneler\n",
    "\n",
    "\\`\\`\\`python\n",
    "# HTTP ve Parser\n",
    "requests\n",
    "beautifulsoup4\n",
    "lxml\n",
    "scrapy\n",
    "\n",
    "# Browser Automation\n",
    "selenium\n",
    "playwright\n",
    "undetected-chromedriver\n",
    "\n",
    "# Yardımcı\n",
    "fake-useragent\n",
    "cloudscraper\n",
    "2captcha-python\n",
    "\\`\\`\\`\n",
    "\n",
    "### 10.2. Regex Patterns\n",
    "\n",
    "\\`\\`\\`python\n",
    "import re\n",
    "\n",
    "# Fiyat\n",
    "price = re.search(r'[\\d,]+\\.?\\d*', text).group()\n",
    "\n",
    "# Email\n",
    "email = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', text)\n",
    "\n",
    "# Telefon (TR)\n",
    "phone = re.findall(r'0\\d{3}\\s?\\d{3}\\s?\\d{4}', text)\n",
    "\\`\\`\\`\n",
    "\n",
    "### 10.3. Önerilen Kaynaklar\n",
    "\n",
    "**Dokümantasyon:**\n",
    "- Scrapy: https://docs.scrapy.org\n",
    "- Selenium: https://selenium-python.readthedocs.io\n",
    "- Playwright: https://playwright.dev/python\n",
    "- BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "**Kitaplar:**\n",
    "- \"Web Scraping with Python\" - Ryan Mitchell\n",
    "- \"Python Web Scraping Cookbook\" - Michael Heydt\n",
    "\n",
    "\n",
    "\n",
    "## Sonuç\n",
    "\n",
    "Web scraping güçlü bir veri toplama yöntemidir ancak:\n",
    "\n",
    "1. **Önce analiz edin** - Site yapısını anlayın\n",
    "2. **Basit başlayın** - En kolay çözümle başlayın\n",
    "3. **Etik olun** - Kurallara uyun\n",
    "4. **Test edin** - Küçük veri setleriyle test edin\n",
    "5. **Sürdürülebilir yapın** - Hata yönetimi ekleyin\n",
    "\n",
    "**En İyi Tavsiye:** Mümkünse API kullanın!\n",
    "\n",
    "---\n",
    "\n",
    "**Güncellenme:** Şubat 2026\n",
    "**Lisans:** Eğitim amaçlı\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
